<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Project Report</title>
  <style>
    body {
      font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
      margin: 0;
      padding: 40px;
      background-color: #f0f4f8;
      color: #333;
    }

    .container {
      background-color: #ffffff;
      padding: 40px;
      max-width: 800px;
      margin: auto;
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
    }

    h1 {
      color: #2a4d69;
      font-size: 2rem;
      margin-bottom: 5px;
    }

    h2 {
      color: #3b6ca2;
      border-bottom: 1px solid #ddd;
      padding-bottom: 5px;
      margin-top: 2em;
    }

    h3 {
      color: #4a6fa5;
      font-size: 1.1rem;
      margin-top: 1.2em;
    }

    p {
      margin-bottom: 1em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      font-size: 0.95rem;
    }

    th, td {
      border: 1px solid #ccc;
      padding: 10px;
      text-align: left;
      vertical-align: top;
    }

    th {
      background-color: #f5faff;
      color: #1a3f5d;
    }

    .section {
      margin-top: 30px;
    }

    figure {
      display: flex;
      flex-direction: column;
      align-items: center;
      margin: 20px auto;
    }

    figcaption {
      font-style: italic;
      color: #555;
      margin-top: 8px;
      text-align: center;
    }

    ol {
      padding-left: 1.2em;
    }

    @media print {
      body {
        background: white;
        color: black;
      }
      .container {
        box-shadow: none;
        border-radius: 0;
      }
      a {
        color: black;
        text-decoration: none;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>VR-407 Revolutionizing Communication with AI</h1>
    <p><strong>Subtitle / Tagline</strong></p>
    <p><strong>Author(s):</strong> Ho Jia En Crystal</p>
    <p><strong>Major:</strong> Biomedical Engineering, Second Major in Data Analytics</p>

    <div class="section">
      <h2>1. Background </h2>
      <p>Non-speaking is the medical term to classify those who are not able to use spoken language to communicate. However, they can still use other forms of communication. Typically, they use Augmentative Alternative Communication (AAC) to communicate with their caregivers. For the disabled, AAC intervention led to 89% of them demonstrating gains in speech production. In addition, AAC methods have also shown to improve the quality of lives for patients with voice loss. [1]


        To empathize with the non-speaking community, I partnered with organizations supporting individuals with Autism, Down Syndrome, Dementia, and Motor Neuron Disease (MND). Interviews with speech therapists and teaching staff provided deeper insights into current communication tools, their effectiveness, and suitability for each group. This approach helped identify strengths and limitations of existing AAC tools, guiding potential improvements.
        
        For the individuals I have met, with Autism, Down syndrome and Dementia, they use forms of low-tech AAC to communicate with their caregivers. This is due to them being able to utilise various forms of physical movements. On the other hand, individuals with MND tend to use high-tech AAC to communicate due to their limited physical ability. Although low-tech AAC may give the impression to be slower and less effective compared to high-tech AAC, interviews with speech therapists highlighted that the choice of communication tool largely depends on personal suitability and preference. Therefore, I chose MND as the primary target group, given their potential to benefit significantly from AI-enhanced AAC and their dissatisfaction with current tools.
        
        This report explores how AI-powered AAC systems, in particular screen-based eye tracking and Mixed Reality platforms, can enhance communication for individuals with MND.
        
  

      </p>
      <h3>1.1 Overview of Non-Verbal Communication and Augmentative Alternative Communication</h3>
      <figure>
        <img src="assets/Screenshot4.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
        <img src="assets/Screenshoot3.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
        <figcaption>Figure A: Different Groups and their Communication Methods</figcaption>
      </figure>

    </div>

    <div class="section">
      <h2>2. Problem Exploration </h2>
      <h3>2.1 Communication Limitations of MND Patients</h3>
      <p>Motor Neurone Disease (MND) is the degeneration of motor neurons, leading to muscle weakness and eventual paralysis. [2]  Physical impediments that these individuals experience make it difficult to control non-verbal communication such as body positioning, gesturing and touching. Due to muscle weaknesses, their facial expressions can also be difficult to interpret correctly.[3] As such, they face communication issues in both expressing themselves and being understood.</p>
      <h3>2.2 Observations from Field Visit</h3>
      <p>To clarify the needs of individuals with MND , I visited the MND Association (MNDa), an organisation serving individuals from early to late stages of MND.
        I visited two individuals who were in the late stage of MND and observed how they interacted with their preferred AAC tool. Both of them have speech function 0 (loss of useful speech) based on the ALS Functional Rating Scale (ALSFRS). [4] 
        They also have a handwriting function of 0 and are both unable to grip a pen, limiting their ability to communicate and thus, they rely solely on a high tech AAC for communication to their caregivers
        </p>
        <figure>
          <img src="assets/Screenshot5.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
          <img src="assets/Screenshot6.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
          <figcaption>Figure B: Table of observations on how MND Patients communicate</figcaption>
        </figure>
      <h3>2.3 Limitations of Current Existing Assistive Technologies</h3>
      <p>This is the list of limitations due to their physical symptoms and the gap in the current Assistive Technology (AT) for individuals with MND:</p>
      <p> A. Slow communication speed due to letter-by-letter input. </p>
      <p> Most MND patient visits utilize the Microsoft Keyboard which has letter-by-letter input. As our eyes are built to scan visual information and not to perform precise selection, using the eyes to select each letter requires unnaturally fixating on the small target, causing it to be slow. Not only that, but the need also to focus intently on specific areas of the screen multiple times add to the cognitive load of patients with MND, contributing to the discomfort that they face. [6] </p>
      <p> B. The eye gaze technology mistakenly identifies the caregiver instead of the patient, causing a need for recalibration.</p>
      <p>  During observations, it was noted that caregivers were frequently detected within the camera's detection range as they attempted to complete partially composed utterances on the MND patient’s AAC, given their visual access to the device. Similar behaviour has been documented in previous studies. As a result, accidental measurements can happen, where the eye tracker senses another pair of eyes.[7]] In this case, the eye tracking technology recognises the eyes of the caregiver and causes the need for the patient to recalibrate the eye tracking device. [8]
      </p>

      <p>
        Though the patient is paralysed, they will need to make minor movements such as going to the washroom, or when they receive saliva swabbing treatment from their caregiver. Hence, each time that such activities occur, the eye-tracking technology requires recalibration. This experience corroborates with existing research which has shown that slight movements in head can also lead to errors in gaze location estimates, leading to inaccuracies and the need for recalibration. [8], [9]
      </p>
      <p> C. Slow communication by Individuals with MND as they tend to mistype letters occasionally. 

      </p>
      <p>
        During house visits, it was observed that patients occasionally mistyped letters, as the system cannot reliably differentiate between intentional and unintentional eye movements. This is largely due to natural ocular behaviors such as saccades—rapid eye movements between fixation points—and eye drifts, which are small, involuntary shifts away from a fixed target. These phenomena can lead to incorrect fixations, resulting in errors when attempting to select specific letters or commands [5]
        Additionally, eye trackers exhibit systematic errors, the disparity between the average gaze point location and actual fixation by them, causing a lack in accuracy. Eye movement data is inherently noisy, causing the calibration to deteriorate over the course of use per time. [7]
        Furthermore, eye movement is a gross motor skill and less precise. Hence, it is more suitable for scanning as compared to focusing on selecting an object. [10], [11] Hence, the usage is unnatural and requires time to get used to. With mistyping of letters, there's a need for additional inputs done by users to correct it, leading to greater fatigue and frustration.
        During house visits and from feedback from the patients, it was mentioned that bigger screens helped to reduce the occurrence of mistyped letters. However, they still felt constrained by the size of their screen given the physical limitations of the screen size of their laptop computers.
      
      </p>
      <p>D. Individuals with MND lack a portable and mobile way of communication. 

      </p>
      Patients currently need to carry both their eye-gaze bar and Surface Pro computer when communicating outdoors. Even when indoors, they require the full setup positioned directly in front of them to use the system effectively. However, finding a suitable surface or table for setup is often challenging—especially in public or informal environments—since patients with severe paralysis are unable to adjust their position to align with the device’s eye detection range. This limitation significantly impacts the portability, usability, and independence of existing communication systems for users with restricted mobility.
      <p>
        <p> E. Individuals with MND often mentioned eye fatigue due to incompatible screen-based interfaces.

        </p>
        <p>
          During the visits with patients, the most mentioned problem that they face is eye fatigue when using the high tech AAC. They often attributed it to the need to focus on the small buttons on the keyboard. As such, user-friendly interfaces should be utilized to combat eye fatigue. Users also face high cognitive fatigue when using incompatible screen-based interfaces. 

        </p>

      </p>
      <h3>2.4 Value Proposition</h3>
      <p>
        The proposed product hopes to provide value through the following:
      </p>
      <p>
        1. Accurate, quick and comfortable communication 
      </p>
      <p>
        2. Reliable calibration system despite changes in position of caregiver interference
      </p>
      <p> 3. Empowering users with greater independence and flexibility
      </p>
      <p>
        4.	Portable and Mobile communication solution
      </p>
      <h2>3. Product Design Strategy: Design Statement </h2>
      <p>
        This project aims to design an Assistive Technology (AT) that leverages on the physical and cognitive capabilities of Individuals with Motor Neurone Disease, to perform reliable, real-time and effective communication.
      </p>
      <h2>4. Product Design Strategy: Design Specification <Table></Table> </h2>
<p>
  To execute a fruitful yet systematic design process, a list of detailed design demands and specifications were constructed from observations and used to aid in each individual component prototyping in the table below.
</p>
<figure>
  <img src="assets/Design_Spec_Tab.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure C: Design Specification Table</figcaption>
</figure>
    </div>

    <div class="section">
      <h2>5. Current Market Alternatives</h2>
      <p>This section provides an overview of existing solutions available in the market, focusing on AAC Application Interfaces as well as Eye Gaze Input Devices. The evaluation aims to determine the strengths and weaknesses of each alternative, identifying areas for improvement that could enhance usability for TAs with MND. These insights will also inform the concept generation plan in the subsequent section.
      </p>
      <h3> 5.1 Current Market Alternatives for AAC Application 
      </h3>
      <p>This subsection evaluates existing app interfaces for AAC solutions, focusing on usability, accessibility, and specific features that are critical for users with MND. 
        The current solutions in the market will be evaluated against the functional requirements that the assistive technology should have in the previous section.
        WaveTalk and Proloquo2Go are both AAC (Augmentative and Alternative Communication) solutions currently available in the market. WaveTalk is an AI powered AAC which aims to enhance communication for individuals with Cerebral Palsy. On the other hand, Proloquo2Go is an award winning, symbol-based AAC app designed for individuals with speech difficulties, offering a highly customizable and intuitive interface for building phrases and sentences using symbols, providing effective communication for various environments.
    
      </p>
      <figure>
        <img src="assets/MA_AAC.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
        <figcaption>Figure D: Market Alternatives for AAC Applications</figcaption>
      </figure>
<p>
  WaveTalk is superior to Proloquo2go when assessing the factor of ergonomic features as well as predictive text capabilities with its unique AI AutoCompletion System.

Currently with WaveTalk being highly catered to individuals with Cerebral Palsy who relies on using a joystick, connected to the application for selection. Individuals with Cerebral Palsy have more control over their gross motor as compared to their fine motor . [12] On the other hand, individuals with MND who use eyes to select buttons on the screen, utilise fine motor skills. With the nature of gross motor skills being different from fine motor skills, there is a need for a change in interface for individuals with MND.

</p>
<h3>
5.2 Current Market Alternatives for Eye-Tracking Devices

</h3>
<p>

  The Tobii PCEye, a stationary eyetracker and Meta Quest Pro, a Mixed Reality (MR) Headset are two market alternatives worth assessing for this section.
With Tobii PCeye being claimed as a compact stationary eye tracker which specifically targets those with disabilities. Like that of the EyeGaze 5 used currently by most MND patients in Singapore. It is a stationary eye tracker that must be mounted onto the computer screen, allowing the user to perform selections on the computer using their eyes. 
On the other hand, Meta Quest Pro also allows for eye tracking, with in built sensors to detect the movement of the eyes, allowing for eye tracking. While its eye-tracking feature is not specifically designed for applications catering to those with disabilities, the possibility of integrating Meta Quest Pro into a solution system is viable as it has been proven to perform eye tracking with high accuracy. 

</p>
<figure>
  <img src="assets/MA_input1.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <img src="assets/MA_input2.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure E: Market Alternatives for Eye Tracking Solutions</figcaption>
</figure>
    </div>

    <div class="section">
      <h2>6. Concept Generation Plan</h2>
      <h3>6.1 Concept Selection Plan</h3>
      <p>To ensure that problems faced by Individuals with Motor Neurone Disease are met, these two concepts are chosen: </p>
<p><u>Screen Based Communication System:
</u>
</p>
<p> <u>The  Screen Based Communication System</u>, utilises a Stationary Eye-tracker and Computer (Figure Y) and comprising of:
<p>
  1.	AITalk, a backend algorithm providing AI AutoComplete Sentences, cycled through OpenAI for faster sentence completion
</p>
<p> 2. Screen-Based User Interface for efficient communication

</p>
<p>
  This is summarised in the flowchart below. 
</p>
<figure>
  <img src="assets/DiagSB.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure F: Interaction Diagram of Screen Based Communication System</figcaption>
</figure>
<p>
  The aim of this system is to find the best way of efficient communication, by modifying the screen-based user interface, and tapping on the AITalk back-end algorithm. 
</p>
<p><u>
  Mixed Reality (MR) Based Communication System:
</u>
</p>
<p>
  <u>The  Mixed Reality Based Communication System</u>, utilises the Meta Quest Pro Headset and comprising of:
</p>
<p>
  1. AITalk, a backend algorithm providing AI AutoComplete Sentences, cycled through OpenAI for faster sentence completion
</p>
<p>
  2. Mixed Reality-Based User Interface for efficient communication
</p>
<p>
  This is summarised in the flowchart below. 
</p>
<figure>
  <img src="assets/DIagMR.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure G: Interaction Diagram of Mixed Reality Based Communication System</figcaption>
</figure>
<p>
  The aim of this system is to experiment with the possibility of communication using eye gestures, using mixed reality. With findings in the market alternatives for input devices, the mixed reality headset has been conceptually proven to be of higher accuracy than stationary eye-trackers and is able to combat the issue that most MND patients have mentioned , where the buttons are often too small. This is due to Mixed reality provides an environment where the buttons are not constrained by the computer screen size, hence allowing for larger button size.

</p>
<p>
  Both systems will utilise AITalk, a backend algorithm that allows for AIAutoCompletion of sentences to aid with efficient communication. 
</p>
<p>
  With working on these 2 concepts, this project aims to find out the best modality of communication for patients diagnosed with Motor Neurone Disease. In section the Results of the tests will be discussed and analysed to compare the 2 different proposed modalities alongside the current traditional method of communicating. 
</p>
<h3>
  6.2 Detailed Design of AITalk (Back-end Algorithm)
</h3>
<p>
  Both the Screen Based system and MR Based System utilises the backend algorithm, AITalk (adapted from WaveTalk – Figure H ) for the eye inputs.


</p>
<figure>
  <img src="assets/WaveTalk.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure H: WaveTalk Application</figcaption>
</figure>
<p>
  AITalk will work on the concept of navigating through core words (making up 80%) of a sentence and fringe words (making up the remaining 20%). Core words are verbs, adjectives and pronouns. On the other hand, fringe words are situation-specific or people-specific words. With these words chosen as input by users, it is cycled into OpenAi for AI AutoCompletion of sentences to allow individuals to complete sentences in a fast and efficient manner, without the need to select each letter on a traditional keyboard to construct a sentence. 


</p>
<p>
  While AITalk (adapted from WaveTalk) has been proven to allow for a 44.6% time reduction in communication for patients with Cerebral Palsy who use a joystick to navigate the WaveTalk Application, it is yet to be explored if it would be suitable for the target group of this report – Patients with Motor Neurone Disease, due to the difference in method of inputs. There is a fundamental difference in input methods – Cerebral Palsy patients interact with the application via hand-controlled joysticks whereas MND patients rely on eye tracking. Each input method comes with its own unique advantages and challenges. As a result, there is a need to study if the back-end algorithm is compatible for users with MND in aiding efficient communication. 


</p>
<h4>
  6.2.1 Subject Selection and Performance Metrics
</h4>
<p>
  With the concept selection and aim, it is essential to recruit test subjects for the study. For this study, 10 healthy test subjects, selected from a pool of peers and family members were recruited. 2 subjects were recruited from the age range of 21-30, 31-40, 41-50, 51-60 and 61-70 years old.  This is to emulate the current situation of MND affecting a large range of ages between 20 to 70 years old. [16]
</p>
<p>
  For the performance metrics, interviews with patients diagnosed with MND as well as staff from MNDa revealed that individuals with MND prioritise:
</p>
<p>
  1. The effiency and speed of typing (Quantitative)
</p>
<p>
  2. Comfortable Interface when typing (Qualitative)
</p>
<p>

  As such, these metrics will be used to assess the performance of the prototypes in the subsequent sections. The test subjects will also be constant throughout the prototypes in this report. 

</p>
<h4>
  6.2.2 Prototype Creating Process on AITalk 
</h4>
<p>
  To develop the prototype, an interactive interface (Figure I) was created using Figma, with the primary aim of testing the compatibility of sentence prediction features when using the eyes as the input modality.
</p>
<figure>
  <img src="assets/figI.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure I: Interactive Interface and Flow (1 to 6)</figcaption>
</figure>
<p>
  The prototype setup involved connecting the Tobii Eye Tracker 5 to a computer. The Tobii Eye Tracker 5 is often loaned to MND patients by the MNDa as seen in figure J.
</p>
<figure>
  <img src="assets/FJ.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure J: Set-up of Prototype</figcaption>
</figure>
<p>
  With the Figma prototype displayed on the screen, users were able to make selections by looking at specific elements, as the eye tracker captured and translated their gaze gestures into interactive inputs on the interface. With reference to Figure J, to select, the user must look at the right click toolbar on the top of the screen for 2 seconds, then at the intended target for another 2 seconds to register a selection.
</p>
<p>
  All participants were instructed to complete specific selection tasks (e.g., selecting “I”, “Jamal”, and “Pen”, the tick button, and the intended sentence) using only their eye movements. The process was repeated across multiple test cases, with timing, accuracy, and qualitative feedback recorded for each.
</p>
<p>
  The Selection Interaction System allows the user to navigate the whole interface (in Figure I). The implementation is as follows: When the user selects an intended option, the Figma Prototyping interaction will navigate to the next page, using the interaction trigger “On Click” .
</p>
<figure>
  <img src="assets/FL.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure K: Interaction process of Selection Interaction System</figcaption>
</figure>
<p>
  To ensure the accuracy of the Figma prototypes that will also be used in the subsequent sections, the Error handling interaction system was implemented in all Figma Prototypes.
</p>
<p>
  The <b>Error Handling Interaction System</b> offers visual feedback when an incorrect option is selected, displaying an error screen for 3 seconds before allowing the user to go back to the screen that they were on before. This built-in delay function acts as a buffer to accommodate potential inaccuracies in eye gesture-based selections. This mimics real-life typing behaviour – where users would need to backspace if they selected a wrong letter on their Microsoft Assistive Keyboard, ensuring that the screen-based prototype is realistic and mimics the reality of the device usage.
</p>
<p>
  Its implementation is as follows: 
</p>
<p>
  When the user mis-selects an option, the Figma Prototyping interaction will navigate to the “error page” as seen below in Figure L. Using the interaction trigger “After Delay”  of 3000 ms, the prototype will allow the user to go back to the page he was at before the mis-selection. 
</p>
<figure>
  <img src="assets/FK.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure L: Error Implementation System</figcaption>
</figure>
<p>
  To create the prototype, I used the current ChatGPT 4o for prediction capabilities. With the prompt “I am someone with MND, type the first 5 sentences that come to mind when I use the Pronoun, Name and Noun”. With that 3 test cases were identified and shown in the table below. (Figure M)
</p>
<figure>
  <img src="assets/FM1.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <img src="assets/FM2.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure M: Generated Outputs with OpenAI with Comparison to Expected Output</figcaption>
</figure>
<p>

  With these 3 test cases, though the expected outputs in mind are not the exactly the same as the sentences predicted by OpenAI based on the Pronoun, Name and Noun. Yet, the sentences highlighted in the table still convey a similar meaning as the expected output and would be able to be used more or less interchangeably. Hence, communication is not adversely affected.

</p>
<p>
  Test Case 1 and 2 illustrates the case when the first 5 outputs of OpenAI are satisfactory. Whereas Test Case 3 illustrates the case when the first 5 outputs of OpenAI are not satisfactory, and there is a need for another set of outputs to be generated. In that particular Test Case, it took up to the 7th output for an output similar to that of the expected output to be found.
</p>
<h4>
  6.2.3 Testing Results on AITalk 
</h4>
<p>
  The setup is as shown in the picture below, with the Tobii Eyetracker connected to the computer. To select, the user has to look at the right click toolbar on the top of the screen for 2 seconds, then at the intended target for another 2 seconds to register an input.
</p>
<figure>
  <img src="assets/FJ.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure N: Set-up for EyeTracking </figcaption>
</figure>
<p>
  Participants were asked to select the Pronoun, Noun and Name, before selecting the tick button to generate the first 5 sentences predicted by OpenAI. This is done using their eyes.  In the case that none of the sentences match the expected output in mind, they will select the restart button to generate another 5 more outputs from OpenAI. 
To test the compatibility of the algorithm in helping with completion of sentences for users using eyes as a modality of selection and input, the mean amount of time taken for each test case was calculated.
The amount of time taken to type out each test cases’ expected output, letter by letter, using the Microsoft assistive keyboard and using the same eye-gaze technology was also taken.
This serves as a comparison between the mean amount of time taken when using AITalk and when using Traditional typing methods.  
</p>
<figure>
  <img src="assets/FO.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure O: Assessing the Compatibility of AITalk (WaveTalks’ Backend Algorithm) with Eye Gesture.</figcaption>
</figure>

<figure>
  <img src="assets/FP.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure P : Analysis of the Compatibility of AITalk (WaveTalks’ Backend Algorithm) with Eye Gesture.

  </figcaption>
</figure>
<p>
  Across all three test cases, the AITalk system consistently resulted in a lower mean time taken compared to traditional typing methods. This can be seen by a reduction in timing of over 55% in all test cases. 
</p>
<p>
  Standard deviation analysis has revealed that test case 3 has the highest standard deviation in timing. This increased variation is likely due to the additional step required. In Test case 3, all the options had to be scanned through, before a decision was made to generate another 5 more outputs. This process requires more cognitive load than test case 1 and 2 where test subjects were able to find the needed output within the first 5 generated outputs, without the need to see the next 5. 
  This study has also revealed that older users (>50 years old) take a longer time for selection and this likely contributed to the increased standard deviation in Test case 3. This could be due to age-related factors such as lesser familiarity with digital interfaces or slower processing speed.
   
</p>
<p>
  This highlights the importance of customising a simple interface for the elderly, with lesser options to prevent cognitive overload and to better suit their preferences. For these users, simplicity and speed of access to a small set of essential phrases will take priority over complex sentence-building features. This also corroborates with interviews with MNDa where they mentioned that the often had a limited set of words of phrases they wish to communicate. For younger users, aged less then 50,  there was a significantly reduction in time taken for sentence construction. This was supported by comments from staff working in MNDa, who through observation feedbacked that this method of communicating was indeed very efficient.  

</p>
<p>
  Given the advantage of AITalk over traditional typing methods, it will be employed in both the Screen Based Prototype and Mixed Reality Based Prototype in the subsequent sections.
</p>
<h3>
  6.3 Screen Based Communication System
</h3>
<p>
  With the Screen Based Communication System consisting of:
</p>
<p>
  1. Screen-Based User Interface
</p>
<p>
  2. AITalk Backend Algorithm , mentioned in section 6
</p>
<figure>
  <img src="assets/DiagSB.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure Q : System of Screen Based Communication System
  </figcaption>
</figure>
<h4>6.3.1 Detailed Design of User Interface for Screen Based Communication</h4>
<p>
  Following the design demand and specifications, while AITalk has been proven to allow for a significant reduction in timing (as mentioned in section 6), it is crucial for the user interface for the Screen Based Communication System to allow for greater efficiency, and comfort when paired alongside the backend algorithm – AITalk. 
As such, the screen-based user interface must be modified to better fit the needs of patients with MND who use eyes as a modality of input. 
After interviews with patients with MND, and MNDa, this can be done by firstly increasing the accuracy and speed of constructing sentences for them and secondly reducing eye strain for them. 

</p>
<p>
  As such, I propose the following:
</p>
<h4>
  6.3.2 Button Positioning which reduces User Fatigue
</h4>
<p>
  I propose to place buttons in the AAC interface which are easily accessible for users and cause the least amount of difficulty when selecting. 
</p>
<h4>
  6.3.3 Layout which reduces User Fatigue
</h4>
<p>
  The ideal layout aims to allow for greater speed when communicating for patients with MND. An ideal layout is important to help improve the usability of the user interface, reducing the errors and confusions generated by the user's interaction with it and can significantly increase the speed of task execution by the user. [17]
</p>
<h4> 
  6.3.4	Button Sizing and Spacing which allows for accurate selection for users using their eyes for selection
</h4>
<p>
  The ideal button sizing and spacing is one that allows for the greatest speed of typing by the users alongside the highest rate of accuracy. This can cause a trade-off between the number of options that is displayed on the screen for users to select.
</p>
<h4>
  6.3.5	Colour Theme which is comfortable for the User using their eyes for selection  
</h4>
<p>
  The colour scheme is crucial for users of eyetracking, who spend prolonged periods infront of the screen. This is due to certain colours being less straining on the eyes. Not only that, colours are also crucial to draw the attention of users eyes to buttons which could potentially promote better focus and accuracy when selection. 
</p>
<h2>
  7.Product Prototyping
</h2>
<h3>
  7.1 Prototyping of Screen based User Interface
</h3>
<p>
  With the product prototyping and testing being an iterative process, the flowchart below summarises the sequence of prototyping. 

</p>


<figure>
  <img src="assets/FR.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure R : Iterative Prototyping of Screen Based User Interface  </figcaption>
</figure>
<h3>
  7.2 Input Method for Prototype

</h3>
<p>
  The product prototyping of the user interface (screen based) follows the same method as that of Section 6.2.2 whereby it utilises Figma, and participants have to select the 5 buttons. (“I”, “Jamal”, “Pen”, Tick Button, Selected Sentence) It also uses the <b>
    Error Handling Interaction System
  </b> and the<b>
    Selection Interaction System 
  </b> to ensure that user inputs are interpreted accurately. 
</p>
<h3>
  7.3 Button Positioning 
</h3>
<h4>
  7.3.1 Product Prototyping of Button Positioning 
</h4>
<p>
  The prototype entails the layout being split into 24 different buttons as shown below and is constructed using Figma. 
</p>
<figure>
  <img src="assets/FS.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure S: 24 Buttons on Figma    </figcaption>
</figure>
<p>

  It is programmed to guide participants through a fixed selection path (eg. Selecting A1, C3, B6 in order), where the button to be selected will be highlighted in green to indicate the next target to select.



</p>
<figure>
  <img src="assets/FT.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure T: Button Positioning Interacting Prototype  </figcaption>
</figure>
<p>

  The aim of this prototype is to identify spaces on the screen which would be the hardest for users to reach when using eye gestures, such that interactable buttons will then be avoided in these spaces.
  Using Figma and Tobii Eye Tracker 5, participants were instructed to select highlighted buttons on a 24-button grid, using only their eyes. The Error Handling Interaction System ensured that accidental selections were accounted for by introducing a delay screen before allowing participants to return to the original task.
  

</p>

<figure>
  <img src="assets/FS.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure S: 24 Buttons on Figma</figcaption>
</figure>
<p>

  To evaluate this, test cases were designed to ensure that all 24 buttons on the user interface are selected and distributed across three key screen zones:

</p>
<p>
  •	Corners: A1, A6, D1, D6 
</p>
<p>
  •	Sides: B1, C1 (left side), B6, C6 (right side)
</p>
<p>
  •	Centre: All remaining buttons located within the inner area of the grid (e.g., B2–B5, C2–C5)
</p>
<p>

  The test cases are summarised in the table below. These tests ensures that all buttons will be selected by the user, to get their feedback on the fatigue based on the zone that they are in. Each test case is designed to ensure that the participant selects at least one corner, one side, and one centre button. This allows the participant to qualitatively compare the difficulty of selecting buttons from different areas within the same test case.


</p>

<figure>
  <img src="assets/FU.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure U: Path for button selection to test for Ideal Button Positioning</figcaption>
</figure>

<h4>
  7.3.2 Product Testing and Analysis of Button Positioning 
</h4>
<p>
  After each test case, the participants were required to record which Button was the most difficult to select with their eyes, and which was the easiest to select with their eyes. 
</p>

<figure>
  <img src="assets/FV.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure V: Table illustrating the most difficult button to select for Participants </figcaption>
</figure>
<h4>7.3.3 Analysis and Insights of Button Positioning Prototype

</h4>
<figure>
  <img src="assets/FQ.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure Q: Table illustrating the most difficult button to select for Majority Buttons </figcaption>
</figure>

<p>
  Test Case 1 has revealed that most test subjects felt that the top row (Row A) was difficult for selection. With a minority stating that the bottom row (D5) was difficult to perform selection. With that, more participants found the top corner element (A6) to be easier for selection as compared to just the sides (A4) in the top row. 
  Test Case 2 has revealed that the left and right sides of the screen were difficult to perform selection, and this was more difficult than selecting a button in the corner (A6). 
  Test Case 3 has also similarly revealed that most participants for the top row, side element (A2) to be difficult to select. While some participants did mention that the bottom row was hard for selection, results have shown that the bottom side button (D2) was harder to select than the bottom corner button (D6). 
  Test Case 4 has revealed that all the participants found the top row was hard for selection, with half of them stating that button A3 was hard for selection and the other half chose button A4. This was in comparison to other buttons placed at the extreme right side (B6) and buttons placed in the centre (B3 and B4). 
  Test Case 5 has revealed that majority of the participants (6 in
   10) found the top side button (A5) to be hard to select whereas 4 in 10 chose leftmost side button (C1). 
  With that, it is possible to tell that most participants found the top sides followed by the bottom sides to be the most difficult to select, followed along by the leftmost and rightmost sides, then the corners and centre elements. As such, it is noted to position information instead of interactive elements at the top row. 
  


</p>
<h3>7.4 Screen Layout </h3>
<h4>7.4.1 Product Prototyping of Layout </h4>
<p>
  The above section has identified that the buttons on the top row proved to be the hardest to select using eye tracking on the screen-based user interface. As such, the difficulties of the top row were taken into consideration when testing out the two layouts. Hence, the buttons for selecting words were placed below the top row. The two screens were made with different layouts, the vertical layout (on the left) and the linear layout (on the right) as seen in Figure R.
</p>
<figure>
  <img src="assets/FigR.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure R: Vertical (left) and Linear (right) layout</figcaption>
</figure>
<p>
  Using Figma, the Vertical Layout and Horizontal Layout was designed. 
  To test the prototype, the user will have to select the buttons “I” , “Jamal”  and “Pen” using their eyes. After which, they would click the “Regenerate” button to regenerate the suggested response one time before selecting the “Tick” button. The selection interaction and the error handling interaction as mentioned in Section 6.2.2 was brought to this prototype, hence ensuring that it replicates a real world usage as much as possible. This standardised “Five Button Test” would also be used to test the button sizing, button spacing and colour theme prototypes.
  To see which layout allows for the most efficient typing, the time taken for the user to complete the “Five Button Test” was recorded.
  

</p>
<h4>
  7.4.2 Product Testing and Analysis of Layout 
</h4>
<p>
A. Qualitative Testing

</p>
<p>
When asked to choose which they felt was easier to navigate, 7 in 10 of the participants preferred the vertical layout.  Most participants mentioned that they appreciated the clear separation of categories into columns which was more intuitive as compared to categories being separated into rows in the linear layout. 
Some mentioned that it is familiar as it resembled the vertical scrolling when using their mobile phones, which is akin to vertical scanning when they are trying to select a word within a particular category. Some also mentioned that it follows the natural reading flow for sentence building, by looking from left to right from Pronoun, Names and Noun, hence is more intuitive.

</p>
<p>
  B. Quantitative Testing
</p>
<p>
  To test out the efficiency of the layout, the time taken for the participants to complete the “Five Button Test” was recorded in the table below. 

</p>
<figure>
  <img src="assets/FigS.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure S: Analysis of Results of Vertical and Linear Layout
    
  </figcaption>
</figure>

































</p>
      <h3>4.2 Technologies Used</h3>
      <p>[List any technologies, e.g. AI, wearable sensors]</p>
    </div>

    <div class="section">
      <h2>5. Prototype Description</h2>
      <h3>5.1 Features of EyeAssist</h3>
      <p>[Explain the key features of the prototype]</p>
    </div>

    <div class="section">
      <h2>6. Testing and Results</h2>
      <h3>6.1 Testing Procedures</h3>
      <p>[Describe how tests were conducted]</p>

      <h3>6.2 Results Summary</h3>
      <table>
        <thead>
          <tr>
            <th>Test Case</th>
            <th>Description</th>
            <th>Outcome</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td>[What was tested]</td>
            <td>[Result]</td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="section">
      <h2>7. Limitations</h2>
      <h3>7.1 Known Issues</h3>
      <p>[List any technical or practical limitations]</p>

      <h3>7.2 Visual Overview</h3>
      <figure>
        <img src="assets/12345.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
        <figcaption>Figure 1: Sketch of the initial prototype for EyeAssist wearable.</figcaption>
      </figure>
    </div>

    <div class="section">
      <h2>8. Future Work</h2>
      <h3>8.1 Next Steps</h3>
      <p>[What will be done next: improvements, scaling]</p>
    </div>

    <div class="section">
      <h2>9. References</h2>
      <ol>
        <li>[Reference 1]</li>
        <li>[Reference 2]</li>
      </ol>
    </div>
  </div>
</body>
</html>
