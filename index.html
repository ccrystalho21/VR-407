<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Project Report</title>
  <style>
    body {
      font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
      margin: 0;
      padding: 40px;
      background-color: #f0f4f8;
      color: #333;
    }

    .container {
      background-color: #ffffff;
      padding: 40px;
      max-width: 800px;
      margin: auto;
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
    }

    h1 {
      color: #2a4d69;
      font-size: 2rem;
      margin-bottom: 5px;
    }

    h2 {
      color: #3b6ca2;
      border-bottom: 1px solid #ddd;
      padding-bottom: 5px;
      margin-top: 2em;
    }

    h3 {
      color: #4a6fa5;
      font-size: 1.1rem;
      margin-top: 1.2em;
    }

    p {
      margin-bottom: 1em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      font-size: 0.95rem;
    }

    th, td {
      border: 1px solid #ccc;
      padding: 10px;
      text-align: left;
      vertical-align: top;
    }

    th {
      background-color: #f5faff;
      color: #1a3f5d;
    }

    .section {
      margin-top: 30px;
    }

    figure {
      display: flex;
      flex-direction: column;
      align-items: center;
      margin: 20px auto;
    }

    figcaption {
      font-style: italic;
      color: #555;
      margin-top: 8px;
      text-align: center;
    }

    ol {
      padding-left: 1.2em;
    }

    @media print {
      body {
        background: white;
        color: black;
      }
      .container {
        box-shadow: none;
        border-radius: 0;
      }
      a {
        color: black;
        text-decoration: none;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>VR-407 Revolutionizing Communication with AI</h1>
    <p><strong>Subtitle / Tagline</strong></p>
    <p><strong>Author(s):</strong> Ho Jia En Crystal</p>
    <p><strong>Major:</strong> Biomedical Engineering, Second Major in Data Analytics</p>

    <div class="section">
      <h2>1. Background </h2>
      <p>Non-speaking is the medical term to classify those who are not able to use spoken language to communicate. However, they can still use other forms of communication. Typically, they use Augmentative Alternative Communication (AAC) to communicate with their caregivers. For the disabled, AAC intervention led to 89% of them demonstrating gains in speech production. In addition, AAC methods have also shown to improve the quality of lives for patients with voice loss. [1]


        To empathize with the non-speaking community, I partnered with organizations supporting individuals with Autism, Down Syndrome, Dementia, and Motor Neuron Disease (MND). Interviews with speech therapists and teaching staff provided deeper insights into current communication tools, their effectiveness, and suitability for each group. This approach helped identify strengths and limitations of existing AAC tools, guiding potential improvements.
        
        For the individuals I have met, with Autism, Down syndrome and Dementia, they use forms of low-tech AAC to communicate with their caregivers. This is due to them being able to utilise various forms of physical movements. On the other hand, individuals with MND tend to use high-tech AAC to communicate due to their limited physical ability. Although low-tech AAC may give the impression to be slower and less effective compared to high-tech AAC, interviews with speech therapists highlighted that the choice of communication tool largely depends on personal suitability and preference. Therefore, I chose MND as the primary target group, given their potential to benefit significantly from AI-enhanced AAC and their dissatisfaction with current tools.
        
        This report explores how AI-powered AAC systems, in particular screen-based eye tracking and Mixed Reality platforms, can enhance communication for individuals with MND.
        
  

      </p>
      <h3>1.1 Overview of Non-Verbal Communication and Augmentative Alternative Communication</h3>
      <figure>
        <img src="assets/Screenshot4.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
        <img src="assets/Screenshoot3.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
        <figcaption>Figure A: Different Groups and their Communication Methods</figcaption>
      </figure>

    </div>

    <div class="section">
      <h2>2. Problem Exploration </h2>
      <h3>2.1 Communication Limitations of MND Patients</h3>
      <p>Motor Neurone Disease (MND) is the degeneration of motor neurons, leading to muscle weakness and eventual paralysis. [2]  Physical impediments that these individuals experience make it difficult to control non-verbal communication such as body positioning, gesturing and touching. Due to muscle weaknesses, their facial expressions can also be difficult to interpret correctly.[3] As such, they face communication issues in both expressing themselves and being understood.</p>
      <h3>2.2 Observations from Field Visit</h3>
      <p>To clarify the needs of individuals with MND , I visited the MND Association (MNDa), an organisation serving individuals from early to late stages of MND.
        I visited two individuals who were in the late stage of MND and observed how they interacted with their preferred AAC tool. Both of them have speech function 0 (loss of useful speech) based on the ALS Functional Rating Scale (ALSFRS). [4] 
        They also have a handwriting function of 0 and are both unable to grip a pen, limiting their ability to communicate and thus, they rely solely on a high tech AAC for communication to their caregivers
        </p>
        <figure>
          <img src="assets/Screenshot5.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
          <img src="assets/Screenshot6.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
          <figcaption>Figure B: Table of observations on how MND Patients communicate</figcaption>
        </figure>
      <h3>2.3 Limitations of Current Existing Assistive Technologies</h3>
      <p>This is the list of limitations due to their physical symptoms and the gap in the current Assistive Technology (AT) for individuals with MND:</p>
      <p> A. Slow communication speed due to letter-by-letter input. </p>
      <p> Most MND patient visits utilize the Microsoft Keyboard which has letter-by-letter input. As our eyes are built to scan visual information and not to perform precise selection, using the eyes to select each letter requires unnaturally fixating on the small target, causing it to be slow. Not only that, but the need also to focus intently on specific areas of the screen multiple times add to the cognitive load of patients with MND, contributing to the discomfort that they face. [6] </p>
      <p> B. The eye gaze technology mistakenly identifies the caregiver instead of the patient, causing a need for recalibration.</p>
      <p>  During observations, it was noted that caregivers were frequently detected within the camera's detection range as they attempted to complete partially composed utterances on the MND patient’s AAC, given their visual access to the device. Similar behaviour has been documented in previous studies. As a result, accidental measurements can happen, where the eye tracker senses another pair of eyes.[7]] In this case, the eye tracking technology recognises the eyes of the caregiver and causes the need for the patient to recalibrate the eye tracking device. [8]
      </p>

      <p>
        Though the patient is paralysed, they will need to make minor movements such as going to the washroom, or when they receive saliva swabbing treatment from their caregiver. Hence, each time that such activities occur, the eye-tracking technology requires recalibration. This experience corroborates with existing research which has shown that slight movements in head can also lead to errors in gaze location estimates, leading to inaccuracies and the need for recalibration. [8], [9]
      </p>
      <p> C. Slow communication by Individuals with MND as they tend to mistype letters occasionally. 

      </p>
      <p>
        During house visits, it was observed that patients occasionally mistyped letters, as the system cannot reliably differentiate between intentional and unintentional eye movements. This is largely due to natural ocular behaviors such as saccades—rapid eye movements between fixation points—and eye drifts, which are small, involuntary shifts away from a fixed target. These phenomena can lead to incorrect fixations, resulting in errors when attempting to select specific letters or commands [5]
        Additionally, eye trackers exhibit systematic errors, the disparity between the average gaze point location and actual fixation by them, causing a lack in accuracy. Eye movement data is inherently noisy, causing the calibration to deteriorate over the course of use per time. [7]
        Furthermore, eye movement is a gross motor skill and less precise. Hence, it is more suitable for scanning as compared to focusing on selecting an object. [10], [11] Hence, the usage is unnatural and requires time to get used to. With mistyping of letters, there's a need for additional inputs done by users to correct it, leading to greater fatigue and frustration.
        During house visits and from feedback from the patients, it was mentioned that bigger screens helped to reduce the occurrence of mistyped letters. However, they still felt constrained by the size of their screen given the physical limitations of the screen size of their laptop computers.
      
      </p>
      <p>D. Individuals with MND lack a portable and mobile way of communication. 

      </p>
      Patients currently need to carry both their eye-gaze bar and Surface Pro computer when communicating outdoors. Even when indoors, they require the full setup positioned directly in front of them to use the system effectively. However, finding a suitable surface or table for setup is often challenging—especially in public or informal environments—since patients with severe paralysis are unable to adjust their position to align with the device’s eye detection range. This limitation significantly impacts the portability, usability, and independence of existing communication systems for users with restricted mobility.
      <p>
        <p> E. Individuals with MND often mentioned eye fatigue due to incompatible screen-based interfaces.

        </p>
        <p>
          During the visits with patients, the most mentioned problem that they face is eye fatigue when using the high tech AAC. They often attributed it to the need to focus on the small buttons on the keyboard. As such, user-friendly interfaces should be utilized to combat eye fatigue. Users also face high cognitive fatigue when using incompatible screen-based interfaces. 

        </p>

      </p>
      <h3>2.4 Value Proposition</h3>
      <p>
        The proposed product hopes to provide value through the following:
      </p>
      <p>
        1. Accurate, quick and comfortable communication 
      </p>
      <p>
        2. Reliable calibration system despite changes in position of caregiver interference
      </p>
      <p> 3. Empowering users with greater independence and flexibility
      </p>
      <p>
        4.	Portable and Mobile communication solution
      </p>
      <h2>3. Product Design Strategy: Design Statement </h2>
      <p>
        This project aims to design an Assistive Technology (AT) that leverages on the physical and cognitive capabilities of Individuals with Motor Neurone Disease, to perform reliable, real-time and effective communication.
      </p>
      <h2>4. Product Design Strategy: Design Specification <Table></Table> </h2>
<p>
  To execute a fruitful yet systematic design process, a list of detailed design demands and specifications were constructed from observations and used to aid in each individual component prototyping in the table below.
</p>
<figure>
  <img src="assets/Design_Spec_Tab.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure C: Design Specification Table</figcaption>
</figure>
    </div>

    <div class="section">
      <h2>5. Current Market Alternatives</h2>
      <p>This section provides an overview of existing solutions available in the market, focusing on AAC Application Interfaces as well as Eye Gaze Input Devices. The evaluation aims to determine the strengths and weaknesses of each alternative, identifying areas for improvement that could enhance usability for TAs with MND. These insights will also inform the concept generation plan in the subsequent section.
      </p>
      <h3> 5.1 Current Market Alternatives for AAC Application 
      </h3>
      <p>This subsection evaluates existing app interfaces for AAC solutions, focusing on usability, accessibility, and specific features that are critical for users with MND. 
        The current solutions in the market will be evaluated against the functional requirements that the assistive technology should have in the previous section.
        WaveTalk and Proloquo2Go are both AAC (Augmentative and Alternative Communication) solutions currently available in the market. WaveTalk is an AI powered AAC which aims to enhance communication for individuals with Cerebral Palsy. On the other hand, Proloquo2Go is an award winning, symbol-based AAC app designed for individuals with speech difficulties, offering a highly customizable and intuitive interface for building phrases and sentences using symbols, providing effective communication for various environments.
    
      </p>
      <figure>
        <img src="assets/MA_AAC.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
        <figcaption>Figure D: Market Alternatives for AAC Applications</figcaption>
      </figure>
<p>
  WaveTalk is superior to Proloquo2go when assessing the factor of ergonomic features as well as predictive text capabilities with its unique AI AutoCompletion System.

Currently with WaveTalk being highly catered to individuals with Cerebral Palsy who relies on using a joystick, connected to the application for selection. Individuals with Cerebral Palsy have more control over their gross motor as compared to their fine motor . [12] On the other hand, individuals with MND who use eyes to select buttons on the screen, utilise fine motor skills. With the nature of gross motor skills being different from fine motor skills, there is a need for a change in interface for individuals with MND.

</p>
<h3>
5.2 Current Market Alternatives for Eye-Tracking Devices

</h3>
<p>

  The Tobii PCEye, a stationary eyetracker and Meta Quest Pro, a Mixed Reality (MR) Headset are two market alternatives worth assessing for this section.
With Tobii PCeye being claimed as a compact stationary eye tracker which specifically targets those with disabilities. Like that of the EyeGaze 5 used currently by most MND patients in Singapore. It is a stationary eye tracker that must be mounted onto the computer screen, allowing the user to perform selections on the computer using their eyes. 
On the other hand, Meta Quest Pro also allows for eye tracking, with in built sensors to detect the movement of the eyes, allowing for eye tracking. While its eye-tracking feature is not specifically designed for applications catering to those with disabilities, the possibility of integrating Meta Quest Pro into a solution system is viable as it has been proven to perform eye tracking with high accuracy. 

</p>
<figure>
  <img src="assets/MA_input1.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <img src="assets/MA_input2.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure E: Market Alternatives for Eye Tracking Solutions</figcaption>
</figure>
    </div>

    <div class="section">
      <h2>6. Concept Generation Plan</h2>
      <h3>6.1 Concept Selection Plan</h3>
      <p>To ensure that problems faced by Individuals with Motor Neurone Disease are met, these two concepts are chosen: </p>
<p><u>Screen Based Communication System:
</u>
</p>
<p> <u>The  Screen Based Communication System</u>, utilises a Stationary Eye-tracker and Computer (Figure Y) and comprising of:
<p>
  1.	AITalk, a backend algorithm providing AI AutoComplete Sentences, cycled through OpenAI for faster sentence completion
</p>
<p> 2. Screen-Based User Interface for efficient communication

</p>
<p>
  This is summarised in the flowchart below. 
</p>
<figure>
  <img src="assets/DiagSB.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure F: Interaction Diagram of Screen Based Communication System</figcaption>
</figure>
<p>
  The aim of this system is to find the best way of efficient communication, by modifying the screen-based user interface, and tapping on the AITalk back-end algorithm. 
</p>
<p><u>
  Mixed Reality (MR) Based Communication System:
</u>
</p>
<p>
  <u>The  Mixed Reality Based Communication System</u>, utilises the Meta Quest Pro Headset and comprising of:
</p>
<p>
  1. AITalk, a backend algorithm providing AI AutoComplete Sentences, cycled through OpenAI for faster sentence completion
</p>
<p>
  2. Mixed Reality-Based User Interface for efficient communication
</p>
<p>
  This is summarised in the flowchart below. 
</p>
<figure>
  <img src="assets/DIagMR.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure G: Interaction Diagram of Mixed Reality Based Communication System</figcaption>
</figure>
<p>
  The aim of this system is to experiment with the possibility of communication using eye gestures, using mixed reality. With findings in the market alternatives for input devices, the mixed reality headset has been conceptually proven to be of higher accuracy than stationary eye-trackers and is able to combat the issue that most MND patients have mentioned , where the buttons are often too small. This is due to Mixed reality provides an environment where the buttons are not constrained by the computer screen size, hence allowing for larger button size.

</p>
<p>
  Both systems will utilise AITalk, a backend algorithm that allows for AIAutoCompletion of sentences to aid with efficient communication. 
</p>
<p>
  With working on these 2 concepts, this project aims to find out the best modality of communication for patients diagnosed with Motor Neurone Disease. In section the Results of the tests will be discussed and analysed to compare the 2 different proposed modalities alongside the current traditional method of communicating. 
</p>
<h3>
  6.2 Detailed Design of AITalk (Back-end Algorithm)
</h3>
<p>
  Both the Screen Based system and MR Based System utilises the backend algorithm, AITalk (adapted from WaveTalk – Figure H ) for the eye inputs.


</p>
<figure>
  <img src="assets/WaveTalk.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure H: WaveTalk Application</figcaption>
</figure>
<p>
  AITalk will work on the concept of navigating through core words (making up 80%) of a sentence and fringe words (making up the remaining 20%). Core words are verbs, adjectives and pronouns. On the other hand, fringe words are situation-specific or people-specific words. With these words chosen as input by users, it is cycled into OpenAi for AI AutoCompletion of sentences to allow individuals to complete sentences in a fast and efficient manner, without the need to select each letter on a traditional keyboard to construct a sentence. 


</p>
<p>
  While AITalk (adapted from WaveTalk) has been proven to allow for a 44.6% time reduction in communication for patients with Cerebral Palsy who use a joystick to navigate the WaveTalk Application, it is yet to be explored if it would be suitable for the target group of this report – Patients with Motor Neurone Disease, due to the difference in method of inputs. There is a fundamental difference in input methods – Cerebral Palsy patients interact with the application via hand-controlled joysticks whereas MND patients rely on eye tracking. Each input method comes with its own unique advantages and challenges. As a result, there is a need to study if the back-end algorithm is compatible for users with MND in aiding efficient communication. 


</p>
<h4>
  6.2.1 Subject Selection and Performance Metrics
</h4>
<p>
  With the concept selection and aim, it is essential to recruit test subjects for the study. For this study, 10 healthy test subjects, selected from a pool of peers and family members were recruited. 2 subjects were recruited from the age range of 21-30, 31-40, 41-50, 51-60 and 61-70 years old.  This is to emulate the current situation of MND affecting a large range of ages between 20 to 70 years old. [16]
</p>
<p>
  For the performance metrics, interviews with patients diagnosed with MND as well as staff from MNDa revealed that individuals with MND prioritise:
</p>
<p>
  1. The effiency and speed of typing (Quantitative)
</p>
<p>
  2. Comfortable Interface when typing (Qualitative)
</p>
<p>

  As such, these metrics will be used to assess the performance of the prototypes in the subsequent sections. The test subjects will also be constant throughout the prototypes in this report. 

</p>
<h4>
  6.2.2 Prototype Creating Process on AITalk 
</h4>
<p>
  To develop the prototype, an interactive interface (Figure I) was created using Figma, with the primary aim of testing the compatibility of sentence prediction features when using the eyes as the input modality.
</p>
<figure>
  <img src="assets/figI.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure I: Interactive Interface and Flow (1 to 6)</figcaption>
</figure>
<p>
  The prototype setup involved connecting the Tobii Eye Tracker 5 to a computer. The Tobii Eye Tracker 5 is often loaned to MND patients by the MNDa as seen in figure J.
</p>
<figure>
  <img src="assets/FJ.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure J: Set-up of Prototype</figcaption>
</figure>
<p>
  With the Figma prototype displayed on the screen, users were able to make selections by looking at specific elements, as the eye tracker captured and translated their gaze gestures into interactive inputs on the interface. With reference to Figure J, to select, the user must look at the right click toolbar on the top of the screen for 2 seconds, then at the intended target for another 2 seconds to register a selection.
</p>
<p>
  All participants were instructed to complete specific selection tasks (e.g., selecting “I”, “Jamal”, and “Pen”, the tick button, and the intended sentence) using only their eye movements. The process was repeated across multiple test cases, with timing, accuracy, and qualitative feedback recorded for each.
</p>
<p>
  The Selection Interaction System allows the user to navigate the whole interface (in Figure I). The implementation is as follows: When the user selects an intended option, the Figma Prototyping interaction will navigate to the next page, using the interaction trigger “On Click” .
</p>
<figure>
  <img src="assets/FL.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure K: Interaction process of Selection Interaction System</figcaption>
</figure>
<p>
  To ensure the accuracy of the Figma prototypes that will also be used in the subsequent sections, the Error handling interaction system was implemented in all Figma Prototypes.
</p>
<p>
  The <b>Error Handling Interaction System</b> offers visual feedback when an incorrect option is selected, displaying an error screen for 3 seconds before allowing the user to go back to the screen that they were on before. This built-in delay function acts as a buffer to accommodate potential inaccuracies in eye gesture-based selections. This mimics real-life typing behaviour – where users would need to backspace if they selected a wrong letter on their Microsoft Assistive Keyboard, ensuring that the screen-based prototype is realistic and mimics the reality of the device usage.
</p>
<p>
  Its implementation is as follows: 
</p>
<p>
  When the user mis-selects an option, the Figma Prototyping interaction will navigate to the “error page” as seen below in Figure L. Using the interaction trigger “After Delay”  of 3000 ms, the prototype will allow the user to go back to the page he was at before the mis-selection. 
</p>
<figure>
  <img src="assets/FK.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <figcaption>Figure L: Error Implementation System</figcaption>
</figure>
<p>
  To create the prototype, I used the current ChatGPT 4o for prediction capabilities. With the prompt “I am someone with MND, type the first 5 sentences that come to mind when I use the Pronoun, Name and Noun”. With that 3 test cases were identified and shown in the table below. (Figure M)
</p>
<figure>
  <img src="assets/FM1.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <img src="assets/FM2.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure M: Generated Outputs with OpenAI with Comparison to Expected Output</figcaption>
</figure>
<p>

  With these 3 test cases, though the expected outputs in mind are not the exactly the same as the sentences predicted by OpenAI based on the Pronoun, Name and Noun. Yet, the sentences highlighted in the table still convey a similar meaning as the expected output and would be able to be used more or less interchangeably. Hence, communication is not adversely affected.

</p>
<p>
  Test Case 1 and 2 illustrates the case when the first 5 outputs of OpenAI are satisfactory. Whereas Test Case 3 illustrates the case when the first 5 outputs of OpenAI are not satisfactory, and there is a need for another set of outputs to be generated. In that particular Test Case, it took up to the 7th output for an output similar to that of the expected output to be found.
</p>
<h4>
  6.2.3 Testing Results on AITalk 
</h4>
<p>
  The setup is as shown in the picture below, with the Tobii Eyetracker connected to the computer. To select, the user has to look at the right click toolbar on the top of the screen for 2 seconds, then at the intended target for another 2 seconds to register an input.
</p>
<figure>
  <img src="assets/FJ.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure N: Set-up for EyeTracking </figcaption>
</figure>
<p>
  Participants were asked to select the Pronoun, Noun and Name, before selecting the tick button to generate the first 5 sentences predicted by OpenAI. This is done using their eyes.  In the case that none of the sentences match the expected output in mind, they will select the restart button to generate another 5 more outputs from OpenAI. 
To test the compatibility of the algorithm in helping with completion of sentences for users using eyes as a modality of selection and input, the mean amount of time taken for each test case was calculated.
The amount of time taken to type out each test cases’ expected output, letter by letter, using the Microsoft assistive keyboard and using the same eye-gaze technology was also taken.
This serves as a comparison between the mean amount of time taken when using AITalk and when using Traditional typing methods.  
</p>
<figure>
  <img src="assets/FO.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure O: Assessing the Compatibility of AITalk (WaveTalks’ Backend Algorithm) with Eye Gesture.</figcaption>
</figure>

<figure>
  <img src="assets/FP.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure P : Analysis of the Compatibility of AITalk (WaveTalks’ Backend Algorithm) with Eye Gesture.

  </figcaption>
</figure>
<p>
  Across all three test cases, the AITalk system consistently resulted in a lower mean time taken compared to traditional typing methods. This can be seen by a reduction in timing of over 55% in all test cases. 
</p>
<p>
  Standard deviation analysis has revealed that test case 3 has the highest standard deviation in timing. This increased variation is likely due to the additional step required. In Test case 3, all the options had to be scanned through, before a decision was made to generate another 5 more outputs. This process requires more cognitive load than test case 1 and 2 where test subjects were able to find the needed output within the first 5 generated outputs, without the need to see the next 5. 
  This study has also revealed that older users (>50 years old) take a longer time for selection and this likely contributed to the increased standard deviation in Test case 3. This could be due to age-related factors such as lesser familiarity with digital interfaces or slower processing speed.
   
</p>
<p>
  This highlights the importance of customising a simple interface for the elderly, with lesser options to prevent cognitive overload and to better suit their preferences. For these users, simplicity and speed of access to a small set of essential phrases will take priority over complex sentence-building features. This also corroborates with interviews with MNDa where they mentioned that the often had a limited set of words of phrases they wish to communicate. For younger users, aged less then 50,  there was a significantly reduction in time taken for sentence construction. This was supported by comments from staff working in MNDa, who through observation feedbacked that this method of communicating was indeed very efficient.  

</p>
<p>
  Given the advantage of AITalk over traditional typing methods, it will be employed in both the Screen Based Prototype and Mixed Reality Based Prototype in the subsequent sections.
</p>
<h3>
  6.3 Screen Based Communication System
</h3>
<p>
  With the Screen Based Communication System consisting of:
</p>
<p>
  1. Screen-Based User Interface
</p>
<p>
  2. AITalk Backend Algorithm , mentioned in section 6
</p>
<figure>
  <img src="assets/DiagSB.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure Q : System of Screen Based Communication System
  </figcaption>
</figure>
<h4>6.3.1 Detailed Design of User Interface for Screen Based Communication</h4>
<p>
  Following the design demand and specifications, while AITalk has been proven to allow for a significant reduction in timing (as mentioned in section 6), it is crucial for the user interface for the Screen Based Communication System to allow for greater efficiency, and comfort when paired alongside the backend algorithm – AITalk. 
As such, the screen-based user interface must be modified to better fit the needs of patients with MND who use eyes as a modality of input. 
After interviews with patients with MND, and MNDa, this can be done by firstly increasing the accuracy and speed of constructing sentences for them and secondly reducing eye strain for them. 

</p>
<p>
  As such, I propose the following:
</p>
<h4>
  6.3.2 Button Positioning which reduces User Fatigue
</h4>
<p>
  I propose to place buttons in the AAC interface which are easily accessible for users and cause the least amount of difficulty when selecting. 
</p>
<h4>
  6.3.3 Layout which reduces User Fatigue
</h4>
<p>
  The ideal layout aims to allow for greater speed when communicating for patients with MND. An ideal layout is important to help improve the usability of the user interface, reducing the errors and confusions generated by the user's interaction with it and can significantly increase the speed of task execution by the user. [17]
</p>
<h4> 
  6.3.4	Button Sizing and Spacing which allows for accurate selection for users using their eyes for selection
</h4>
<p>
  The ideal button sizing and spacing is one that allows for the greatest speed of typing by the users alongside the highest rate of accuracy. This can cause a trade-off between the number of options that is displayed on the screen for users to select.
</p>
<h4>
  6.3.5	Colour Theme which is comfortable for the User using their eyes for selection  
</h4>
<p>
  The colour scheme is crucial for users of eyetracking, who spend prolonged periods infront of the screen. This is due to certain colours being less straining on the eyes. Not only that, colours are also crucial to draw the attention of users eyes to buttons which could potentially promote better focus and accuracy when selection. 
</p>
<h2>
  7.Product Prototyping
</h2>
<h3>
  7.1 Prototyping of Screen based User Interface
</h3>
<p>
  With the product prototyping and testing being an iterative process, the flowchart below summarises the sequence of prototyping. 

</p>


<figure>
  <img src="assets/FR.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure R : Iterative Prototyping of Screen Based User Interface  </figcaption>
</figure>
<h3>
  7.2 Input Method for Prototype

</h3>
<p>
  The product prototyping of the user interface (screen based) follows the same method as that of Section 6.2.2 whereby it utilises Figma, and participants have to select the 5 buttons. (“I”, “Jamal”, “Pen”, Tick Button, Selected Sentence) It also uses the <b>
    Error Handling Interaction System
  </b> and the<b>
    Selection Interaction System 
  </b> to ensure that user inputs are interpreted accurately. 
</p>
<h3>
  7.3 Button Positioning 
</h3>
<h4>
  7.3.1 Product Prototyping of Button Positioning 
</h4>
<p>
  The prototype entails the layout being split into 24 different buttons as shown below and is constructed using Figma. 
</p>
<figure>
  <img src="assets/FS.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure S: 24 Buttons on Figma    </figcaption>
</figure>
<p>

  It is programmed to guide participants through a fixed selection path (eg. Selecting A1, C3, B6 in order), where the button to be selected will be highlighted in green to indicate the next target to select.



</p>
<figure>
  <img src="assets/FT.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure T: Button Positioning Interacting Prototype  </figcaption>
</figure>
<p>

  The aim of this prototype is to identify spaces on the screen which would be the hardest for users to reach when using eye gestures, such that interactable buttons will then be avoided in these spaces.
  Using Figma and Tobii Eye Tracker 5, participants were instructed to select highlighted buttons on a 24-button grid, using only their eyes. The Error Handling Interaction System ensured that accidental selections were accounted for by introducing a delay screen before allowing participants to return to the original task.
  

</p>

<figure>
  <img src="assets/FS.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure S: 24 Buttons on Figma</figcaption>
</figure>
<p>

  To evaluate this, test cases were designed to ensure that all 24 buttons on the user interface are selected and distributed across three key screen zones:

</p>
<p>
  •	Corners: A1, A6, D1, D6 
</p>
<p>
  •	Sides: B1, C1 (left side), B6, C6 (right side)
</p>
<p>
  •	Centre: All remaining buttons located within the inner area of the grid (e.g., B2–B5, C2–C5)
</p>
<p>

  The test cases are summarised in the table below. These tests ensures that all buttons will be selected by the user, to get their feedback on the fatigue based on the zone that they are in. Each test case is designed to ensure that the participant selects at least one corner, one side, and one centre button. This allows the participant to qualitatively compare the difficulty of selecting buttons from different areas within the same test case.


</p>

<figure>
  <img src="assets/FU.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure U: Path for button selection to test for Ideal Button Positioning</figcaption>
</figure>

<h4>
  7.3.2 Product Testing and Analysis of Button Positioning 
</h4>
<p>
  After each test case, the participants were required to record which Button was the most difficult to select with their eyes, and which was the easiest to select with their eyes. 
</p>

<figure>
  <img src="assets/FV.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure V: Table illustrating the most difficult button to select for Participants </figcaption>
</figure>
<h4>7.3.3 Analysis and Insights of Button Positioning Prototype

</h4>
<figure>
  <img src="assets/FQ.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure Q: Table illustrating the most difficult button to select for Majority Buttons </figcaption>
</figure>

<p>
  Test Case 1 has revealed that most test subjects felt that the top row (Row A) was difficult for selection. With a minority stating that the bottom row (D5) was difficult to perform selection. With that, more participants found the top corner element (A6) to be easier for selection as compared to just the sides (A4) in the top row. 
  Test Case 2 has revealed that the left and right sides of the screen were difficult to perform selection, and this was more difficult than selecting a button in the corner (A6). 
  Test Case 3 has also similarly revealed that most participants for the top row, side element (A2) to be difficult to select. While some participants did mention that the bottom row was hard for selection, results have shown that the bottom side button (D2) was harder to select than the bottom corner button (D6). 
  Test Case 4 has revealed that all the participants found the top row was hard for selection, with half of them stating that button A3 was hard for selection and the other half chose button A4. This was in comparison to other buttons placed at the extreme right side (B6) and buttons placed in the centre (B3 and B4). 
  Test Case 5 has revealed that majority of the participants (6 in
   10) found the top side button (A5) to be hard to select whereas 4 in 10 chose leftmost side button (C1). 
  With that, it is possible to tell that most participants found the top sides followed by the bottom sides to be the most difficult to select, followed along by the leftmost and rightmost sides, then the corners and centre elements. As such, it is noted to position information instead of interactive elements at the top row. 
  


</p>
<h3>7.4 Screen Layout </h3>
<h4>7.4.1 Product Prototyping of Layout </h4>
<p>
  The above section has identified that the buttons on the top row proved to be the hardest to select using eye tracking on the screen-based user interface. As such, the difficulties of the top row were taken into consideration when testing out the two layouts. Hence, the buttons for selecting words were placed below the top row. The two screens were made with different layouts, the vertical layout (on the left) and the linear layout (on the right) as seen in Figure R.
</p>
<figure>
  <img src="assets/FigR.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure R: Vertical (left) and Linear (right) layout</figcaption>
</figure>
<p>
  Using Figma, the Vertical Layout and Horizontal Layout was designed. 
  To test the prototype, the user will have to select the buttons “I” , “Jamal”  and “Pen” using their eyes. After which, they would click the “Regenerate” button to regenerate the suggested response one time before selecting the “Tick” button. The selection interaction and the error handling interaction as mentioned in Section 6.2.2 was brought to this prototype, hence ensuring that it replicates a real world usage as much as possible. This standardised “Five Button Test” would also be used to test the button sizing, button spacing and colour theme prototypes.
  To see which layout allows for the most efficient typing, the time taken for the user to complete the “Five Button Test” was recorded.
  

</p>
<h4>
  7.4.2 Product Testing and Analysis of Layout 
</h4>
<p>
A. Qualitative Testing

</p>
<p>
When asked to choose which they felt was easier to navigate, 7 in 10 of the participants preferred the vertical layout.  Most participants mentioned that they appreciated the clear separation of categories into columns which was more intuitive as compared to categories being separated into rows in the linear layout. 
Some mentioned that it is familiar as it resembled the vertical scrolling when using their mobile phones, which is akin to vertical scanning when they are trying to select a word within a particular category. Some also mentioned that it follows the natural reading flow for sentence building, by looking from left to right from Pronoun, Names and Noun, hence is more intuitive.

</p>
<p>
  B. Quantitative Testing
</p>
<p>
  To test out the efficiency of the layout, the time taken for the participants to complete the “Five Button Test” was recorded in the table below. 

</p>
<figure>
  <img src="assets/FigS.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
 
  <figcaption>Figure S: Analysis of Results of Vertical and Linear Layout

  </figcaption>
</figure>
<p>

  While the vertical and linear layout had almost the same amount of mean time taken, the linear layout has a larger standard deviation than that of the vertical. The higher variation in timing for linear layout means that some participants took a significantly longer time to navigate the linear layout, possibly due to difficulties in scanning across horizontal rows, or higher accuracy errors when doing so. 
  It also suggests that the vertical layout offers a more reliable and stable experience across a wider range of users. 
  

</p>
<h3>
  7.5 Button Sizing
</h3>
<h4>
  7.5.1 Product Prototyping of Button Sizing 
</h4>
<p>
  To ensure that the study can be applicable for different interfaces, or even possibly integrated into the Mixed Reality headset, the button sizing was varied based on the percentage of the screen that it occupies. The aim of this prototype is to find the ideal button sizing, to maximise accuracy for a user when performing selection using eye gesture. 
</p>
<p>
  With the screen size of Figma being 1340x800 in size, the button sizes were calculated accordingly to the area occupied by it. Additionally, the horizontal and vertical button spacing was kept constant even with the change in button sizing to ensure the reliability of the test. The button size percentages constructed were those of 0.5,1,2,3 and 4 % of the screen. The prototypes are as shown in the figure below.
</p>
<figure>
  <img src="assets/FT1.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <img src="assets/FT2.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure T: Prototypes of different Button Sizing  </figcaption>
</figure>
<p>

  The button size percentages were chosen keeping in mind to leave space for the toolbar at the top. The “Five Button Test” was administered, where users select the words order “ I” , “Jamal” and “Pen”, followed by the “Regenerate” and “Tick” button. This was done on the screen-based UI and their eyes were tracked by the Tobii Eyegaze technology.  Participants started off the testing with the smallest button size, and progressively to the largest button size (4%). A break of 5 minutes was given between each test case to ensure that the results will not be affected by eye fatigue. 




</p>
<h4>
  7.5.2 Product Testing and Analysis of Button Sizing

</h4>
<p>

  The mean and standard deviation are recorded in the table below. 
</p>

<figure>
  <img src="assets/FigU.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure U: Analysis from prototype testing</figcaption>
</figure>
<p>
  As seen from the results above, larger buttons lead to a faster performance with a decrease in mean time taken. Additionally, the standard deviation drops significantly from 11.8 seconds for the small buttons to 4.1 seconds for the largest. As the buttons get larger, users consistently take a shorter amount of time for the same selection, meaning that it is easier for them to navigate the interface.
  As such, it can be concluded that the ideal button size to use is from the tests would be the largest one, which occupies 4% of the screen. 
  
</p>
<h3>
  7.6 Button Spacing 
</h3>
<h4>
  7.6.1 Product Prototyping of Horizontal Button Spacing
</h4>
<p>
  This prototype iterates upon the previous sections’ findings and also utilises the buttons which occupy 4% of the screen.
The horizontal spacing between each button was varied while ensuring that the 3 categories: Pronoun, Names and Nouns are still within the screen. Using Figma, and also utilising the selection interaction system and error interaction system as mentioned in Section 6.2.2. The “Five Button Test” was administered, where users select the words order “ I” , “Jamal” and “Pen”, followed by the “Regenerate” and “Tick” button. The aim of this prototype is to find out the ideal horizontal spacing, as too narrow of a spacing may cause mis-selections whereas a spacing too wide may lead to extra time taken due to greater distance for the eye to scan and focus upon the desired word.
The horizontal distance between each button is recorded as a percentage of the screen width, and varied accordingly. This prototype includes the following % horizontal distance : 8,4,2 and 1. The prototypes are shown in the figure below.

</p>
<figure>
  <img src="assets/FigV.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure V: Prototypes of different horizontal spacing</figcaption>
</figure>
<h3>
  7.6.2 Testing and Analysis of Horizontal Spacing Prototype 
</h3>
<p>
  <u>
    Quantitative Study
  </u>
</p>
<p>
  During this study, the time taken for the user to select the 5 buttons were taken. The mean amount of time per scenario was recorded with the standard deviation shown in the table below. 

</p>
<figure>
  <img src="assets/FigW.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure W: Analysis for Prototypes with different horizontal spacing</figcaption>
</figure>

<p>

  As seen in the table above, with the decrease in horizontal distance, there is an increase in mean amount of time taken. Participants have mentioned that it is difficult to select accurately when the buttons are too close to each other. This corroborates with the average number of mis-selections being the highest when the horizontal spacing between button is the lowest. With the standard deviation being relatively constant when varying the horizontal spacing, it shows that user performances were consistent, hence suggesting that the results are reliable. As such from this testing, it can be gathered that the ideal horizontal spacing would be 8% distance of the screen length.
</p>
<h4>
  7.6.3 Product Prototyping of Vertical Button Spacing
</h4>
<p>
  With the findings above, the vertical spacing was varied. 
  With the variation in vertical spacing, the number of buttons on the screen decreased. However, for this prototype, it was ensured that the minimum number of buttons on the screen was 2 and the maximum number was 4. The maximum of 4 was chosen to prevent excessive cognitive load from the user having too many buttons per category. 
  The prototype is made on Figma, and utilising the selection interaction system and error interaction system as mentioned in Section 6.2.1. The aim of this prototype is to find out the ideal vertical spacing, as a spacing too close will cause mis-selections whereas a spacing too wide may lead to extra time due to greater distance for the eye to scan. Additionally, it is to assess the trade-off between vertical spacing and number of choices per category.
  
  The prototypes are as crafted and shown in the figure below. 

</p>
<figure>
  <img src="assets/FigX.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure X: Prototypes of different Vertical spacing

  </figcaption>
</figure>
<h4>
  7.6.4 Product Testing and Analysis of Vertical Button Spacing
</h4>
<p>
  <u>
    Quantitative Testing and Analysis 
  </u>
</p>
<p>
  During this study, the time taken for the user to complete the “Five Button Test” was taken. The mean amount of time per scenario was recorded with the standard deviation shown in the figure below. 

</p>

<figure>
  <img src="assets/FigY.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure Y: Testing Results of prototypes with different Vertical spacing  </figcaption>
</figure>

<p>
  While analysing the mean amount of time taken to complete the “Five Button Test”, though when the vertical spacing is the largest, the mean amount of time taken is the shortest with the least number of mis-selections, there is no clear trend in the other data which shows a pattern between the vertical spacing and also the mean amount of time taken for selection. For the other cases (not including 24% distance of vertical screen), the average number of mis-selections are equal as well, thus it can be said that the difference in vertical spacing does not significantly affect the mean amount of time taken for selection.

</p>
<p>
  <u>
    Qualitative Testing and Analysis 
  </u>
</p>
<p>

  To gain more insights into the data collected, participants were asked to choose the interface that they preferred the most. With that, most of them chose when the vertical spacing occupied 3% of the length of the interface. Most of them commented that although a larger vertical spacing may lead to higher accuracy when selection and potentially higher speed, they would still prefer to have more options within each category. 
Therefore, when qualitatively and quantitively comparing the different vertical spacings, the ideal vertical spacing is of 3% of length of the interface. 

</p>
<h3>
  7.7 Colour Theme
</h3>
<h4>
  7.7.1 Product Prototyping of Colour Theme 
</h4>
<p>
  The prototype was created using Figma, one with a cool colour theme and another with a warm colour theme as shown in the picture below.

</p>
<figure>
  <img src="assets/FigZ.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure Z: Prototypes of different themes (Cool on the left and Warm on the right) </figcaption>
</figure>
<p>
 
It also utilises the selection interaction system and error interaction system as mentioned in Section 6.2.2. The “Five Button Test” was administered, where users select the words order “ I” , “Jamal” and “Pen”, followed by the “Regenerate “and “Tick” button. The aim of this prototype is to find out if warm or a cool colour theme is best suited for eye-based selection. 
 
</p>
<h4>
  7.7.2 Testing and Analysis of Colour Prototyping 
</h4>
<p>
  <u>
    Quantitative Testing and Analysis    
  </u>
</p>
<p>
  During this study, the time taken for the user to complete the “Five Button Test” taken. The mean amount of time per scenario was recorded with the standard deviation shown in the table below.  
</p>
<figure>
  <img src="assets/Fig1.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure 1. Testing and Analysis of Prototype (Colour Theme) </figcaption>
</figure>
<p>
  The Neutral Grey theme serves as a benchmark and is used for comparison between warm and cool colour theme. 

</p>
<p>
  Results have shown that the warm colour theme has a lower mean time taken as compared to the neutral grey and cool colour theme and thus is a more suitable colour theme for eye gesture input. This point is further backed up by the fact that its standard deviation is lower than that of the cool colour theme, indicating that most participants performed more consistently when using the warm colour theme, and the results are reliable. Participants mentioned that with using the warm colour theme, it was easier to focus on the intended selection as they felt that the colour was more attention-grabbing and visually distinct, helping them to quickly identify their target, reducing hesitation and thus decreasing the time taken for selection while still maintaining accuracy. 
  On the other hand, for the cool colour theme, the amount of time taken was higher than that of neutral grey and also warm colour theme.  There were 2 anomalies in the data, leading to a high standard deviation. Participants mentioned that the cool colour theme appears bright in nature and this distracts them from focusing on the selection of the intended button. 
  

</p>
<p>
  As such, the most ideal colour scheme for an eye-based selection on screen interface would be a warm colour theme as opposed to a cool colour theme. 
</p>

<h3>
  7.8 Summary and Prototype of Ideal UI 
</h3>
<p>
  Based on the observations and findings above, the ‘ Ideal screen-based UI’ can be constructed and seen in the image below. This ‘ Ideal Screen-based UI” will be used in the discussion section in the later chapters to assess its performance against traditional typing methods as well as Mixed Reality (MR) based communication.



</p>
<figure>
  <img src="assets/Fig2.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure 2: Ideal Screen-Based User Interface </figcaption>
</figure>
<h2>
  8. Mixed Reality Based Communication
</h2>
<h3>
  8.1 Introduction of Mixed Reality Based Communication
</h3>
<p>

  The Mixed Reality Based communication consists of using the Meta Quest Pro, using eye selections to interact with the mixed reality world. Eye gestures are taken in by the Meta Quest Pro for selection of different buttons. Selection can be done by glancing at an element in the Meta Quest Pro, while wearing the Meta Quest Pro Headset as shown below.


</p>

<figure>
  <img src="assets/Fig3.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure 3: User Wearing the Meta Quest Pro </figcaption>
</figure>
<p>
  The interactions for this communication method is summarised in the flowchart below.  
</p>

<figure>
  <img src="assets/Fig$.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure 4: Overview of Mixed Reality Communication System </figcaption>
</figure>

<h3>
  8.2 Detailed Design of User Interface for Iteration 1.0 of Mixed Reality Based Communication
</h3>
<p>
  Following the design demand and specifications, while AITalk has been proven to allow for a significant reduction in timing (as mentioned in section 6), it is crucial for the user interface for the Mixed Reality Communication System to allow for greater efficiency, and comfort when paired alongside the backend algorithm – AITalk. 
  As such, the Mixed Reality based user interface must be modified to better fit the needs of patients with MND who use eyes as a modality of input. 
  After interviews with patients with MND, and MNDa, this can be done by firstly increasing the accuracy and speed of constructing sentences for MND patients and secondly reducing eye strain for them through the modification of the interface . As such, I propose for an Ideal Mixed Reality User Interface. This will be done with the adaption of the button positioning, layout, button sizing and spacing and colour theme as mentioned in section 8. It assumes that the principles behind Screen-based user interface will be able to be adapted to that of Mixed Reality.
  

</p>
<h3>
  8.3 Iteration 1.0 
</h3>
<h4>
  8.3.1 Product Prototype of Iteration 1.0  
</h4>
<p>
  The prototype was designed, adapting the features of the ‘ Ideal Screen Based User Interface” as mentioned in section 8. It is shown in the Figure below. 
</p>
<figure>
  <img src="assets/Fig5.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure 5: Prototype Iteration 1.0 </figcaption>
</figure>

<p>
  The aim of iteration 1.0 is to construct a user interface that is most efficient for eye-selection in mixed reality.
  To construct a Mixed Reality (MR)-based user interface, the application ShapesXR was used on the Meta Quest Pro headset to design and position the interface elements, including buttons. Using the percentages of button sizing and spacing, as a percentage of the Field of View (FOV) on the Mixed Reality headset, the buttons and layout was constructed in the Mixed Reality using an application called ShapesXR, a design and prototyping system on the headset. 
  

</p>
<p>
  Leveraging the eye-tracking capabilities of the Meta Quest Pro, users are able to select buttons simply by moving their eyes and glancing at the target, enabling intuitive and hands-free interaction in a mixed reality environment.
</p>
<p>
  The <b> Selection Interaction System

  </b> which helps with interactions between buttons—used to trigger transitions to the next output or scene—were implemented using ShapesXR’s built-in "hover and navigate to next scene" interaction system. This allows seamless and natural movement through the interface, simulating real-time gaze-based communication pathways.
</p>
<p>
  The <b>Error Interaction System 

  </b>is also implemented. It plays a crucial role in ensuring that mis-selections are reflected in the total time taken during testing. When a user makes an unintended selection—one that was not required in the study—the system triggers an error screen as shown below that is displayed for 3 seconds before automatically returning the user to their previous screen.
</p>
<p>

  This Error Interaction System is implemented using the “After delay” then “Go to scene” trigger-action pair in ShapesXR. It allows the system to handle errors gracefully while preserving a natural interaction flow.
  The temporary delay not only provides visual feedback to the user but also ensures that such mis-selections are measured as part of the interaction cost, contributing to a more accurate evaluation of user performance.
</p>
<h4>
  8.3.2 Insights from Iteration 1.0
</h4>
<p>
  During the development of the prototype, it was observed that although parameters such as button sizing (% of screen), spacing, layout, and colour theme were adapted from the screen-based UI, the number of buttons displayed in the Mixed Reality (MR) version differed.
  To interact with the mixed reality, users were required to put on the Meta Quest Pro as seen in the photo below. 
  

</p>
<figure>
  <img src="assets/Fig6.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure 6: Usage of MR Headset </figcaption>
</figure>
<p>
  Internal testing revealed that adding more than four buttons per category would push some elements outside the user's Field of View (FoV)—especially for users who cannot move their head, such as individuals with paralysis from Motor Neuron Disease (MND) and who rely solely on eye movement. 
</p>
<p>
  Additionally, during internal testing with a task requiring selection of “I”, “Jamal”, and “Pen”, followed by the tick and selection of sentence, users faced difficulty completing the process. This was due to two main limitations. Firstly, the lack of adjustable dwell time in the ShapesXR application. Secondly, the high sensitivity and calibration precision of eye-tracking in MR.
</p>
<p>
  As a result, users were unable to make consistent and accurate selections, preventing the continuation of the study in its original format.
</p>
<h3>
  8.4 Iteration 2.0 
</h3>
<p>

  To increase usability, the layout of the Mixed Reality user interface was redesigned to avoid accidental mis-selections of buttons in MR, to improve gaze accuracy.

</p>
<h4>
  8.4.1 Detailed Design of Iteration 2.0

</h4>
<p>
  Following the design demand and specifications, while AITalk has been proven to allow for a significant reduction in timing (as mentioned in section 6), it is crucial for the user interface for the Mixed Reality Based Communication System to allow for greater efficiency, and comfort when paired alongside the backend algorithm – AITalk. 
  As such, the Mixed Reality-based user interface must be modified to better fit the needs of patients with MND who use eyes as a modality of input. 
  


</p>
<p>
  After interviews with patients with MND, and MNDa, this can be done by firstly increasing the accuracy and speed of constructing sentences for them and secondly reducing eye strain for them. Additionally with the internal testing on Iteration 1.0 of Mixed Reality Based User interface, the User layout was modified to better fit the Mixed Reality World.

</p>
<p>

  As such, I propose the following:

</p>
<p>
  1. Layout that minimises mis-selections
</p>
<p>
  As seen from iteration 1.0, the mixed reality user interface works differently from the screen-based user interface. This is attributed to the lack of dwell time option – the amount of time the user has to stare at the element for selection, on ShapesXR application. In ShapesXR, once the user glances at the element, it is selected. With that, the buttons were spaced out more evenly and intentionally placed slightly outside the direct centre of the Field of View (FoV). 


</p>
<p>
  2. The Minimiser Button to prevent Midas Touch
</p>
<p>

  I propose the addition of an easily accessible minimise button, which will give the user greater control over when to maximise and minimise the communication surface, thereby reducing frustration and increasing usability. Since the patients use their eyes for selection, it is important for them to be able to survey their surroundings without unintentionally triggering actions. The minimise button will allow them to close the communication interface, and rest their eyes without triggering any unintended actions, addressing the "Midas Touch" issue that often affects eye-gaze users.[18]


</p>
<h4>
  8.4.2 Product Prototype of Iteration 2.0
</h4>
<p>

  The prototype was then made with ShapesXR Application, following the same method as used in Iteration 1.0.
The prototype consists of the same buttons, arranged in a different layout. (Figure 7, left)  It also consisted of a minimising button. The buttons are selected through the eye-based interaction system from ShapesXR, whereby it will navigate to the next scene ‘ On Hover”. For the minimising button located at the top right of the FOV in brown, once the user looks at the button, the communication interface will be minimised, allowing the user to look around the surroundings without the fear of mis-selecting a button as seen below.

</p>

<figure>
  <img src="assets/Fig7.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure 7: Before (left) and After(right) Minimiser button after hovering with eyes </figcaption>
</figure>
<p>

  The prototype consists of the user having to select the Pronoun (located on the left side of the FOV in red), Name (located at the bottom of the FOV in orange) and the Name(located at the right side of the FOV in yellow). After then, to confirm their selection, users press the tick button on the top bar in order to cycle these three words through OpenAI. The flow is as shown in the figure below.

</p>

<figure>
  <img src="assets/Fig8.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>Figure 8: Procedure of Prototype Iteration 2.0 </figcaption>
</figure>

<p>
  The aim of this prototype is to assess the efficiency of MR for eye-based selection and communication. Before the participants tried to look at the first sentence, they would say start, and when they have selected the intended sentence, they would say stop for time-keeping purposes.


</p>
<h4>
  8.4.3 Testing and Analysis of Iteration 2.0 
</h4>
<p>
  <u>
    Quantitative testing and Analysis  
  </u>
</p>
<p>
  With that, participants were asked to try out the MR Headset, whereby they were told to use their eyes to hover and select the required button as seen in the procedure in Figure Q. 
  The mean amount of time taken for the same set of participants to navigate the interface was shown in the table below
 

</p>
<figure>
  <img src="assets/Fig9.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>
    Figure 9: Testing and Analysis of MR Based Communication
     </figcaption>
</figure>
<p>

  
With the low standard deviation, it is evident that the amount of time taken for participants to navigate the screen is consistent, indicating that the screen layout is intuitive for them to navigate. 

</p>
<p>
  <u>
    Qualitative Testing and Analysis 
  </u>
</p>
<p>
  When asked about which is more tiring to navigate, the ideal “ screen-based user interface” communication system or “Mixed reality based user interface” communication system, all of the participants indicated that the mixed reality user interface is less tiring for their eyes to navigate due to the large buttons. This is highly probable due to the fact that the buttons in mixed reality are not constrained by the screen size. 
  Additionally, participants expressed a clear preference for the reduced need for calibration in the MR system. In contrast, many reported needing frequent recalibration between trials while using the screen-based communication interface, which disrupted the flow of interaction and added to their fatigue.
  

</p>

<h2>
  9. Discussion and Analysis of Findings 
</h2>
<p>

  As seen from Figure 10, the Mixed Reality based communication system allows for the lowest amount of time taken, when navigating the same interface and interaction. As such, it can be concluded to be the most efficient way of communication using eyes.

</p>
<figure>
  <img src="assets/Fig10.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>
    Figure 10: Comparison of time taken for different communication methods. </figcaption>
</figure>
<p>
  With reference to section 4 which states the design specification and functional requirements required of the Assistive Technology, the screen-based communication system and mixed reality-based communication system was compared in Figure 11. 

</p>
<figure>
  <img src="assets/Fig11pt1.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
  <img src="assets/Fig11pt2.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">

  <figcaption>
    Figure 11: Criteria Satisfaction Comparison between Screen Based and MR Based Communication System </figcaption>
</figure>
<p>
  As such, it can be concluded that with the MR Communication System being able to address the functional requirements of the assistive technology for MND patients and addressing their problems, it is more suited than the traditional method of communication as well as the Screen based communication system in providing an efficient and comfortable way of communication for MND Patients. 
</p>
<h2>
 10.  Future Works
</h2>
<p>
  While the current system has been tested on healthy individuals, future iterations must consider the specific needs of patients with Motor Neurone Disease (MND). A key feature to implement is a text-to-speech generator, which would allow users to verbalize their selected commands through the MR interface, enabling clearer and
  more accessible communication with caregivers.
  
</p>
<p>
  In addition, further research into improving eye gaze tracking capabilities is recommended. This includes the development of visual feedback mechanisms that help users understand where their gaze is being registered in real time, thereby improving accuracy, confidence, and user control.

</p>
<p>

  After interviews with the Motor Neurone Disease Association (MNDa), its secretary, Ms. Sabrina, shared that the MR communication system appears to be a promising and cost-effective alternative to current technologies. Priced at SGD 1,990 on Amazon, the Meta Quest Pro is significantly more affordable compared to the Tobii PCEye, which costs SGD 4,240 (excluding a computer). She also noted the benefit of the all-in-one wearable format, which supports greater mobility for patients.
</p>
<p>
  However, a key limitation raised was the weight of the headset (~700g), which may not be suitable for patients with a weak neck. To address this, future versions of the system should explore using lighter hardware alternatives, developing support mechanisms to reduce strain on the neck or investigating alternative mounting methods (e.g., headrests or wheelchair-mounted arms)
</p>
<p>

  Ultimately, the long-term goal is for the MR communication system to support a wider population of users with paralysis or severe mobility limitations, providing them with an efficient, portable, and user-friendly communication tool.
</p>
</p>
      <h3>4.2 Technologies Used</h3>
      <p>[List any technologies, e.g. AI, wearable sensors]</p>
    </div>

    <div class="section">
      <h2>5. Prototype Description</h2>
      <h3>5.1 Features of EyeAssist</h3>
      <p>[Explain the key features of the prototype]</p>
    </div>

    <div class="section">
      <h2>6. Testing and Results</h2>
      <h3>6.1 Testing Procedures</h3>
      <p>[Describe how tests were conducted]</p>

      <h3>6.2 Results Summary</h3>
      <table>
        <thead>
          <tr>
            <th>Test Case</th>
            <th>Description</th>
            <th>Outcome</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td>[What was tested]</td>
            <td>[Result]</td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="section">
      <h2>7. Limitations</h2>
      <h3>7.1 Known Issues</h3>
      <p>[List any technical or practical limitations]</p>

      <h3>7.2 Visual Overview</h3>
      <figure>
        <img src="assets/12345.png" alt="Prototype sketch" style="max-width: 80%; border-radius: 8px;">
        <figcaption>Figure 1: Sketch of the initial prototype for EyeAssist wearable.</figcaption>
      </figure>
    </div>

    <div class="section">
      <h2>8. Future Work</h2>
      <h3>8.1 Next Steps</h3>
      <p>[What will be done next: improvements, scaling]</p>
    </div>

    <div class="section">
      <h2>9. References</h2>
      <ol>
        <li>[Reference 1]</li>
        <li>[Reference 2]</li>
      </ol>
    </div>
  </div>
</body>
</html>
